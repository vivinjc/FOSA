 %-----------------------------------------------------------------------------
% Template for seminar 'Program Analysis' at TU Darmstadt.
%
% Adapted from template for sigplanconf LaTeX Class, which is a LaTeX 2e
% class file for SIGPLAN conference proceedings (by Paul C.
% Anagnostopoulos).
%
%-----------------------------------------------------------------------------


\documentclass[authoryear,preprint]{sigplanconf}

% A couple of packages that may be useful
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithm2e}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage[english]{babel}
\usepackage{blindtext}
\usepackage{float} 
\usepackage{graphicx}
\usepackage{caption}

\begin{document}
	\special{papersize=a4}
	\setlength{\pdfpageheight}{\paperheight}
	\setlength{\pdfpagewidth}{\paperwidth}
	
	\title{A survey on systematic construction of benchmarks for Static Analysis tools}
	\subtitle{Supervisor: Michael Eichberg \\ eichberg@cs.tu-darmstadt.de}
	
	
	\authorinfo{Kasra Qasempour Soleimany}{ TU DARMSTADT}{k.qasempour@gmail.com}
	\authorinfo{Vivin Joseph Christoph}{TU DARMSTADT}{vivinjc@gmail.com}	
	\maketitle


\begin{abstract}
	Benchmarks have been used in computer science to compare the performance of computer systems. They provide an experimental basis for evaluating software engineering processes or techniques in an objective and repeatable manner. Static Analysis Tools (SAT) have been become popular recently, and whenever a new tool is developed, it comes with a benchmark to show how the tool performs in comparison to other options. However, there is not unified and unique approach for evaluating these tools. In this paper, we looked over some approaches on how to systematically build a benchmark for SATs. We explained different benchmarking approaches and  the characteristics of a good benchmarking approach. At the end, we summarize the paper by pointing to differences and similarities of these approaches.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Static Analysis Tools (SAT) have been become popular recently. The British Computer Society defines static analysis of source code as ``the analysis of a program carried out without executing the program'' \cite{Glossary_of_term}. Thus the input for a static analysis tools is source code or other artifacts which builds from source code, like binary code. However, there is not a clear list of metrics or definition in order to compare and benchmark these tools. Generally the main issue with static analysis programs is completeness and soundness. A static analysis program is either unable to reliably detect all targeted problems or is prone to false positives, i.e., it reports findings which turn out to be wrong on closer examination \cite{Scanstud}. 

Benchmarks provide an experimental basis for evaluating software engineering theories, represented by software engineering techniques, in an objective and repeatable manner \cite{675631}. Based on IEEE, a benchmark is defined as ``a procedure, problem, or test that can be used to compare systems or components to each other or to a standard" \cite{IEEE1990}. A benchmark represents a problem and possible solutions to it, by defining the motivating comparison, a task sample and evaluation measures \cite{sim2003}.  The task sample can contain programs, tests, and other artifacts dependent on the benchmark’s motivating comparison. A benchmark controls the task sample reducing result variability, increasing repeatability, and providing a basis for comparison \cite{sim2003}.

In this paper we study recent studies on how to systematically create and build benchmarks for static analysis tools and we try to express commonalities, differences, shortcomings and advantages. In the rest of this paper, in Section \ref{sec:sec_approaches} we provide the foundation for benchmarking criterion and what are the characteristics of a good SAT and a benchmark. Next, in Section \ref{sec:sec_scanstud} we study \textbf{Scanstud: A Methodology for Systematic, Fine-grained Evaluation of Static Analysis Tools} \cite{Scanstud}, then we study \textbf{Delta-Bench: Differential Benchmark for Static Analysis Security Testing Tools} \cite{8170097}. Next we see how MuBenchPipe works which proposes \textbf{A Systematic Evaluation of Static API-Misuse Detectors} \cite{8338426}. Then in Section \ref{sec:sec_hermes} we talk about \textbf{Hermes: Assessment and Creation of Effective Test Corpora} \cite{Reif:2017:HAC:3088515.3088523}. Afterwards, in Section ~\ref{sec:sec_differential_benchmark} we look at \textbf{FOSS Version Differentiation as a Benchmark for Static Analysis Security Testing Tools} \cite{Pashchenko:2017:FVD:3106237.3121276} and finally we summerize in Section ~\ref{sec:conclusion}. 

\section{Foundations}
\label{sec:sec_approaches}

In this section we define some basic concepts and definitions that we will use in the rest of paper. 

\subsection{Success Criteria for SATs}
\label{sec:subsec_successc}

Before we dive in to discussing different approaches, we take look over some criteria that a good SAT should meet. They were introduced by Chess and West \cite{Chess:2007:SPS:1406221}:

\begin{itemize}
	\item \textbf{C1 - Quality of the analysis} which indicates the precision of a given tool. Like the language feature coverage, or rate of false negative/positive.
	\item \textbf{C2 - Trade-off between precision and scalability} Like tradeoff between analysis duration and unwanted results, or memory usage vs depth of data flow to capture. 
	\item \textbf{C3 - Vulnerability types coverage} Different types of vulnerabilities is the tool aware of. 
	\item \textbf{C4 - Usability of the tool} How easy the tool can be used.
\end{itemize}

Chess and West introduced these criteria for Static Analysis Security testing tools, so we can generalize the C3 to \textit{``Issue types coverage"}. Except C4, other criteria should be considered in the design of a benchmark. 

\subsection{Benchmarking approaches}
We mentioned that the input of a SAT is source code, and correspondingly the benchmarking approaches are categorized based on the input. Johns and Jodeit define 3 different benchmarking approaches \cite{Scanstud}: 

\begin{itemize}
	\item \textbf{Real-world, vulnerable software} Test on big open source projects which has some known vulnerabilities. The approach is perfect for indicating C2, and the overall tool performance. It has some limitation, for example if a tool fails to find a known issue, what is the reason behind that? Does it stem from C1 or C2? or maybe the tool does not cover the issue, means C3. Another issue is that roughly no real world project can be found which contains all the covered issues of a tool, thus the tool should be tested on different projects therefore needs more effort in comparison to test on 1 project that has all the issues in one place.
	
	\item \textbf{Educational applications} There are many testing application for educational purposes which contain a wide range of known issues, for example for security vulnerabilities OWASP’s WebGoat \cite{owasp} is a good choice. The source of these types of applications can be used as a test target (input to SAT). One of the benefits of these this type is that they cover a comprehensive range of issues in a context. Moreover they are well documented and with regard to size they are small but rich in issues, so the test time is less than the one for real projects. 
	
	\item \textbf{Micro benchmarking suites} contains test suits which each contains one or more issues. The test suits might not even be executable. The approach allows us to easily indicates C1. Also by defining test cases having C3 is not a problem. One of the shortages of this type of benchmarking is how to declare C2, because the size of the tests are way smaller than a real project. 
\end{itemize}

\subsection{Characteristics of a good benchmark approach}
\label{sec:good_benchmark}

In order to compare the upcoming systematic benchmark studies we use some characteristics which are well defined by Johns and Jodeit, and vital for any systematic benchmark approach \cite{Scanstud}:

\begin{itemize}
	\item \textbf{CH1 - Testing tool capabilities individually} a systematic benchmark should be able to test each capability of the tool individually. 
	\item \textbf{CH2 - Easy, correct, and iterative testcase creation} The system should be able to create easy, correct and iterative testcases. Testcases should be short and human readable and the system should allow the manual verification of a certain issue among defined tests. 
	\item \textbf{CH3 - Automatic test execution and result evaluation} The test-code development is not a one-time perfect task. The test cases might not be perfect intially and the test designers will iteratively improve them to fit their needs. Therefore ease the creations and also the benchmark process, the system should be able to automatically and repeatedly run tests and evaluate the results. 
\end{itemize}

Beside these 3 characteristics, we point to 8 more properties for a successful benchmark, the upcoming properties are described in \cite{sim2003}:

 \begin{itemize}
	\item \textbf{Accessibility} A benchmark should be easy to obtain and use. The test materials and results need to be publicly available. Anyone should be able to apply the benchmark to a tool or techniques and compare their results with others. The test and results should be understandable with software engineers at any level of expertise. 

	\item \textbf{Affordability} There should be a trade off between cost and result of benchmark. The cost contains human resource, software and hardware. 

	\item \textbf{Clarity} Benchmark's specification should be clear and concise, in order to easily adopt and extend it.
	
	\item \textbf{Relevance/Representativeness} The collection and the goal of tasks in the benchmark must be selected in a way that system might behave in real world sample. This means they should not be meaningful in just the benchmark settings. Also the performance measures must be relevant to comparison being made. This is not an easy property to satisfy, because the testcases should be representive of real source code, and not full of false alarms, where finding such a test case is not easy. 
	
	\item \textbf{Solvability} Completing the task sample and obtaining correct metrics should be possible. 
	
	\item \textbf{Portability} A benchmark should be usable by different techniques without bias. This means it should specified with enough level of abstractions. As an example, the benchmark may need to implemented more than once for different platforms or architectures. 
	
	\item \textbf{Scalability} It should be scalable to any technique with any level of maturity. Adding new techniques and adding other targets should be possible. For example it should support commercial products beside less mature products like a research prototype. 
	
	\item \textbf{Fairness} A benchmark should not bias towards a specific technique. For example supporting only specific programming language \cite{lu2005bugbench}.
\end{itemize}

\section{Scanstud}
\label{sec:sec_scanstud}
The paper propose a micro benchmarking approach for systematic, fine-grained evaluation of Security Static Analysis Tools. They have provide a system in which tests could be defined, run automatically and repeatedly and evaluate the results. 
In the rest of this section we look over some of the challenges they had and how they solved them. 

One of the important parts in automating a benchmark is evaluating the results. The main challenge is how to map the tools findings with mined issues in tests. Using line number to address the issues is not enough, because different tools propose different line numbers for a certain issue. As an example, take a look at the Listing \ref{ls:sourcec}, the code has an off-bounds writing operation issue. But which line should be reported, line 2 which issue stems from or line 3 which executes the issue. Each of the decision is reasonable so each tool may decide to report line 2 or 3. . They addressed this issue by hosting each \textit{testcode} on a dedicated application. The application only contains a single test case, maybe a real issue or a false positive. They differentiate the \textit{testcode} and \textit{host program}. Host program is the testcode added by some other codes to make it parse-able, compile-able and execute-able. In their test design they have defined to terms, \textit{test} and \textit{testcase}. The testcase is the smallest unit, it is designed to capture only one issue, or to be one false positive. It is passed if the issue found by the tool, or if it is ignored in case of being a false positive. On the other hand, a test contains one or more testcases, and it is passed if all the testcases passed. A test could be used to verify that if a tool can capture a series of issue semantically related. A test could be used to double check a capability of a tool, by defining a vulnerable testcase and a false positive testcase. This way if the test passed, it shows that the tool is working perfectly in finding the issue. The ability to define different testcases and test enables ones to check C1 and C3 criteria mentioned in \ref{sec:subsec_successc} which supports CH1.

\lstset{numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt}
\lstset{basicstyle=\ttfamily}
\lstset{frame=tb}
\lstset{basicstyle=\tiny}

\begin{lstlisting}[float,caption=Off-by-one vulnerability caused by insecure loop condition \cite{Scanstud} ,label=ls:sourcec,language=C]
[...]
for (i = 0; src[i] && (i <= sizeof(dst)); i++) {
	dst[i] = src[i];
}
[...]
\end{lstlisting}

The host program might produce some false positives. To address this issue they create two versions of host program, one with testcode and the other without testcode and they run the preprocessing step on both of them. The results that occur in both of the processes are the false positivies caused by host program and will be removed from result report. 

In order to automatically creating the tests, they have a list of template issues saved in files, for example opening a socket. Then there is a script that could insert each template in code, and automatically create different version of tests. Moreover, they have created scripts for automatic tool execution, result diffing, and result evaluation. This greatly supports CH3.

Scanstud also provides a way to manually verify the issue. This is normally done by running the host program which is a complete application to run and provides means to interact with the testcode. This way test writer can activate the crafted vulnerability to make sure that the written code is insecure. For instance for Java test suite, the test application is indeed a full J2EE application, which use the codes in form of servlet, and then the host program provides a web UI which enables the test designer to access the vulnerable code. This supports CH1.

\section{FAULTBENCH}
\label{sec:sec_FAULTBENCH}
SATs can identify anomalies in source code early in development process. They produce reports which contain potential program anomalies, which are called \textit{alrets}. For each of the prodcued alerts this is the task of developer to determine whether the alert is a real anomaly or \textit{true positive} (TP). When alert is not important to developer, or it does not indicate an anomaly, it is called \textit{False positive} (FP) \cite{ayewah2007evaluating}. SATs produce a huge  number of alerts in their reports, while unfortunately many of these alerts are FPs. There are two well known techniques in order to address the problem of enourmess number of FPs and they are called \textit{FP mitigation techniques}. \textit{Alert prioritization techniques} are a complementary process that prioritize the produced alerts to users by showing the TPs on top of the list. There is also another complementary techniques called  \textit{alert classification techniques} that are able to classify the produced alerts to FPs and TPs. \textit{FAULTBENCH} \cite{heckman2008establishing} aims to propose a benchmark io order to compare the FP mitigation techniques. 

The paper claims that there are several benchmarks available for anomaly detection techniques, but these benchmarks are not fulfill all the needs. First of all, they are mostly accept tools for C/C++ programming languages and also they do not provide an easy way to repeat and automate process. Furthermore in the process of alert priotrization, there is a part that researcher should remove the anomalies, which is very costly in large projects, becuase there are many number of FPs produces. Therefore they want to build an automated repeatable benchmark of small, real Java programs on different domains. 

\subsection{FAULTBENCH creation } 

\textbf{ FAULTBENCH Process} section 3.1

\textbf{ FAULTBENCH definition} section 3.2

\textbf{Subject selection} section 3.4

\textbf{3.5 FAULTBENCHSubjectInitialization} vivin

\textbf{3.6 BenchmarkLimitations} vivin



 For the alert oracles they used alerts produced by FINDBUGS \cite{hovemeyer2004finding}.

\section{MuBenchPipe}
\label{sec:sec_mubenchpipe}

The paper tries to evaluate different Java API-misuse detectors, static, dynamic and hybrid detectors. and therefore developed a MuBenchPipe \cite{8338426} the first automated pipeline to benchmark API-misuse detectors. They have a data set called MuBench, which contains 90 Java API misuses, 73 investigated from real world projects, and 17 from a conducted survey. They have selected 4 tools and they set up different experiments in order to evaluate many aspects of these tools. Some of these projects accept source code as input and others accept binary code, therefore for part of constructing their benchmark, they made binary out of the source codes available in MuBench dataset. 

MuBenchPipe automates many parts of their experiments:
\begin{itemize}
	\item \textbf{checkout}: each test item in MuBench dataset has a recorded Id for the respective commit of the project on version control system. It supports SVN and Git repositories, source archives (zip), also hand-crafted examples that come with MuBench.
	\item \textbf{Compile} For some of the detectors, we need binary as input, therefore MuBenchPipe, is able to build the code by getting all the dependencies and base on the configuration found on MuBench dataset. 
	\item \textbf{Validation} MuBenchPipe automatically release the result to a review website for manual validation. For every detector finding, the website shows the source code it is found in along with any metadata the detector provides, such as the violated pattern, the properties of the violation, and the detector’s confidence\cite{8338426}.
\end{itemize}

It provides a command line interface to control every part, such as retrieval and compilation of target projects, running detectors, and collecting their findings. There is a review website that reviewers can see all the findings. Some of them are labeled as potential and MuBenchPipe will not compute the experiment statistics unless they receive a review by at least 2 reviewers. 

With regard to reproduction of experiments, there is a Docker image for both MuBenchPipe and the review website which make it easier to run experiments on different platforms. The review website allows independent reviews, while reviewers working on different workstations, and ensures the integrity using authentication. It is easy to extend MuBenchPipe facilitates the extension in 2 ways. First it has a simple data schema for misuse examples and new plugins (for detection or evaluation) could be added via a clear java interface with Maven. 

\section{Hermes}
\label{sec:sec_hermes}
The Hermes\cite{Reif:2017:HAC:3088515.3088523} is a tool, that can analyze collection of Java projects and return a optimal set, which covers all requirements, whilst omitting the repetition.This is one of the prime feature of a benechmarking suite, which would satisfy the defined success criteria C1, C2 and C3. Also the characteristics CH1 and relevance would be addressed. In the Qualitas Corpus\cite{5693210} \cite{Dingsoyr:2013:RCL:2507288.2507322} collection considered, from which Hermes tries to extract the most suitable subset, as per the requirement. However collections like Qualitas Corpus and others can be outdated or built for other purposes, or may have features that are no longer relevant. A subset is composed by taking projects from various collections, across domains which would be developed by various people, thus covering an entirety of the specification as mentioned in characteristic relevance. Therefore, these projects vary in style, size and methodology. Hence understanding the properties within each of the project that comprises the subset is difficult. Without this understanding it is not possible to establish conclusive results. Hermes helps to overcome these issues with a two step approach, that is:

\begin{enumerate}
	\item Help to understand the properties within the projects and 
	\item Help build an optimal collection from the projects which satisfies the requirements.
\end{enumerate}

The queries in Hermes can be extended which enables to further optimize and customize the result set. Hermes is extensible and configurable. However, the requirement is that the collection have to be in java bytecode. The collection can be java programs or libraries. Here the extensibility offered by Hermes is noteworthy. As we mentioned, a good benchmarking tool must have a justifiable, tradeoff between precision and scalability as mentioned in success criteria C2. 

Hermes works by searching across the projects looking for the specified feature. Then it collects and sorts the projects that have those specific feature at least once. Hermes is built on OPAL. OPAL can compute and control flow graphs, call graphs, support low level queries, high abstraction, here Scala choco (constraint programming library) is used for selecting optimal collection. Since the tool searches through the collection of libraries to find specified feature it not only helps better understanding of the project set, but also makes sure that the project with the specified feature is chosen from the whole collection.

Hermes Configuration: every selected project has id, class-path and attributes(optional). Queries specified by default are executed. However, their execution can be decided as per the requirements, with possibility to add newer queries. Once queries are executed, Hermes shows if the project contains the feature or not, if the project contains the feature, then the navigation to that particular code portion within the project is enabled. Also these details can be managed enabling easier management of larger collections of projects like Qualitas Corpus. Feature query is a static analysis run on a project which outputs presence of the specified feature or closely related feature. Eg: having only java7 class files. Every feature query has Id of the respective feature and query. Once all the queries are executed, Hermes will select the projects with atleast one occurrence of the feature. If two projects have the same feature occurrence then it will pick the project with lesser number of methods (smaller project). This is niche feature as it tries to keep the overall size of the collection to bare minimum, whilst coveriing the requirement.This is user friendly feature as it helps the user effort to be minimum and hence we can say the criteria C4 is clearly satisfied by the tool. Apart from this the results can be exported to the external file and post processing can be done. This can be a useful feature for academic and scientific purposes.

Some more tool set which analysed were meeting some of the criteria for a successful benchmarking tool. Like, Blackburn’s DaCapo benchmark suite\cite{Blackburn:2006:DBJ:1167473.1167488} is about developing java performance evaluation using static and dynamic software metrics. And, how to develop the collection for the evaluation. Tempero et al’s Qualitas Corpus is about empirical study of code based on size, content, representativeness and permanence. Livshits et al’s SecuriBench\cite{Livshits_defininga} is about static and dynamic security analysis. All these collections are topic specific and are not updated regularly and that is an issue. Dujmovic presented parameterized approach to generate fully synthetic programs that allow benchmarking and testing. But this approach is not applicable to the real world application. Dujmovic et al’s automatic benchmark management\cite{Dujmovic:2010:AGB:1712605.1712654} can extract and update a collection, but it cannot find the differences between the projects. It is important to note that, these above mentioned benchmarking tools are not as comprehensive as Hermes. Hermes tries to solve the problem by becoming one stoop solution, within the benchmarking domain where tools are scarce.

\begin{table*}[htbp]
	\centering
	\captionsetup{justification=centering}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{|c|c|c|c|}
			\hline
			& CH1: Testing tool capabilities individually & CH2: Easy, correct,  and iterative testcase creation & CH3: Automatic test execution and result evaluation \\ \hline
			Scanstud & Possible & Possible & Possible \\ \hline
			Delta-Bench & Possible & No information & No information  \\ \hline
			MuBenchPipe & Possible & No information & Possible  \\ \hline
			Hermes & Possible & Possible & Possible  \\ \hline
			Differential Benchmark & Possible & Partially Possible & No information  \\ \hline
		\end{tabular}%
	}
	\caption{Characteristics available within each approach}
\label{tbl1}
\end{table*}

Evaluation methodology of Hermes : 
\begin{enumerate}
	\item Understanding the test collection and 
	\item Generating effective integration testing suite.
\end{enumerate}

1) Understanding the test collection:
Hermes when run on Qualitas Corpus from 2013, it was able to find out, that none of the project used java 8 features. None of the projects used java FX framework Only one project used java 7 feature. Hereby summarizing that collection from September 2013 cannot be used to test for latest java features.Thus we have a clear proof of Hermes satisfying C1, C2, C3 and characteristics CH1, CH2.

2) Generating effective integration testing suite:
Hermes generated the subset from Qualitas Corpus that is optimal( small, yet effective, means to say it meets the requirement specified) and covering all scenarios mentioned. Whilst creating a optimal subset.Therefore reducing the run time from 16.77 minutes (time required to run on hundred projects that comprises Qualitas Corpus) to 2.82 minutes( time required to run on selected subset). And the code coverage was only 1.09\% less. This could be bettered by adding the missing features in the queries and re-running Hermes. And the size still would remain considerably smaller than original test collection built. Thus Hermes fulfils its purpose of better understanding the test set and automatic creation of subsets using the derived understanding. This fulfils the characteristic CH3.

\begin{table*}[htbp]
	\resizebox{\textwidth}{!}{
		\begin{tabular}{|c|p{3cm}|p{2cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
			\hline
			& Accessibility & Affordability & Clarity & Relevance & Solvability & Portability & Scalability & Fairness \\ \hline
			Scanstud & Not able to find any repository & Yes & Yes & Not easy to judge & Yes & No, they were not able to adopt all tools & Yes & Yes \\ \hline
			Delta-Bench & Yes, they use open source applications & Yes & Not enough info about automation and reproduction & Not easy to judge & Yes & No, they just use tools the supports Java & Yes & It just support Java tools and those that pecifically created for finding security vulnerabilities \\ \hline
			MuBenchPipe & Yes, The MuBench (the dataset) is available online & Yes & Yes & Not easy to judge & Yes & No, for example they can not use DroidAssist because its implementation only supports Dalvik Bytecode & Yes & It just support Java API misses \\ \hline
			Hermess & No, sufficient information is available on the public availability of the benchmark & Yes & Yes, it is a well defined benchmark framework & Yes, it covers the entirety of tests   & Yes, it solves the problem at hand  & Yes & Yes, as it supports further query specifications & No, as it currently only supports java bytecode  \\ \hline
			Differntial Benchmark & No, sufficient information is available on the public availability of the benchmark  & No, because it requires a lot of re runs  & Yes  & Yes, as it tries to address most vulnerabilities individually  & Yes  & Yes, as it is well defined  & Yes  & Yes, as it is not bound to a particular programming language  \\ \hline
		\end{tabular}
	}
	\caption{Properties within each approach}
	\label{tbl2}
\end{table*}

\section{Differential Benchmark}
\label{sec:sec_differential_benchmark}
Both the academic and Industrial surveys do not provide substantial information on the benchmark construction\cite{5954458} \cite{5689543} . Through the project static analysis tool exposition that was run from 2008 to 2016, software assurance reference dataset was created. However, it is important to note, bug collections in the following dataset may not represent all vulnerabilities in the real world\cite{greiman2016iccws} \cite{Kupsch2009ManualVA} \cite{Wilander250309}. This is the reasons for false alarms. False alarm is a background noise that is generated, which can cause attention deviation from the original issue. And this a major source of reason in skipping over certain vulnerabilities during testing.

Juliet test suite, suggests benchmarking real-world application is challenging. And the challenge is due to variability within the real world ecosystem. Dolan gavitt et al \cite{7546498}, suggests method to artificially inject vulnerabilities into source code of real-world application, Large scale automated vulnerability addition (LAVA methodology). However LAVA methodology cannot perform automatic detection and correction of false alarms in real world application. Also this methodology cannot inject some specific vulnerabilities. Cardar and Donaldson \cite{Cadar:2016:APA:2889160.2889206} suggested program transformation like mutation and metamorphic testing to increase code coverage of SAST tools. But this cannot find bugs in the source code of  real world application. Thus the issue with only using SAST tools to analyze real world application is that false alarms for bugs analyzed might be true alarams for bugs that are yet to be analyzed.

So, free open source society (FOSS) \cite{Pashchenko:2017:FVD:3106237.3121276} differential vulnerability fixing methodology was developed to overcome these defects. Here empahsis is on ground truth. Ground truth is the expected correct output. Nuyen et al \cite{Do:2016:TAB:2931021.2931023} suggests to consider code snippet that should produce expected correct output when it had bug and the same snippet after the bug is fixed. Therefore here there are two versions that are being compared which forms the core idea of the evaluation described. This would the fullfil the success criteria C1 and C2 as idea is focused on helps in finding different types of vulnerability with good precision.

Generic methodology for a testing tool evaluation: 
\begin{enumerate}
	\item Run tool on vulnerable version.
	\item Extract tool finding.
	\item Check for correctness in the finding.
\end{enumerate}

This methodology is suitable for custom collection of projects for which, it is generally know where the errors in the code are present. But this is not the case in the real world applications as there might be multiple bugs. Bugs are usually fixed one after the other and therefore the resultant set must have difference only the in the corrected area, compared to the earlier version prior to running the tool on it. 

Proposed methodology of differential benchmarking in FOSS:
\begin{enumerate}
	\item Find the changed lines of code by comparing fixed and version with bug.
	\item Run the tool on the version with the bug
	\item Run the tool on the fixed version
	\item Ignore the tool findings that occur in fixed and version with the bug that are irrelevant to the current bug in consideration, as this can be considered as background noise (false alarms)
	\item Classify the remaining finding
\end{enumerate}

Here only one bug is fixed at a given run, therefore any other finding not related to the discovered bug is ignored even if it might be right. This is major disadvantage, as it leads to redundant reruns. Here characteristic relevance is not satisfied because the tool is not covering the complete bug spectrum, and we have less information to determine the characteristic CH3. However due to the various re-runs conducted features can be tested individually satisfying the CH1 described above.

Synthetic test suite typically contains only flaw within a testcase. Differential benchmarks has sufficient complexity to identify false alerts. Current tools require some additional input data whilst differential benchmark needs only the fixed and bugged version of projects to generate expected correct code. Differential benchmark is independent of programming languages and bug types. Auto classification of true positive, false negative, false positive, and true negative is also possible. Thus we can say the tool might partially satisfy characteristic CH2, as it cannot be said to be easy due to the re-runs required to arrive at the final conclusion.

\section{Conclusion}
\label{sec:conclusion}
To summarize, it can be seen from Table~\ref{tbl1} that in all the approaches it is possible to test tools capabilities individually. On the other hand, iteratively creating easy and correct testcase, is only possible in Hermes and Scanstud. With regard to automatic execution of benchmark, all the approaches can do that, except Delta bench and differential-benchmark.

Table~\ref{tbl2} compares the rest of properties we mentioned in Section~\ref{sec:sec_approaches}. The table shows that all approaches are affordable, solvable and scalable. Approaches are not generally portable, this means not all tools in that context could be tested by approaches. They are generally accessible, the sources, tools are available online.  
 
	\bibliographystyle{abbrvnat}
	\bibliography{references}
	
	\end{document}
